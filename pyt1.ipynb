{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4955, 0.3885],\n",
       "         [0.9609, 0.2982]],\n",
       "\n",
       "        [[0.7603, 0.4584],\n",
       "         [0.4031, 0.4334]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor Basics\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.rand(2,2,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zeros\n",
    "xx = torch.zeros(3,3, dtype=torch.int)\n",
    "xx\n",
    "\n",
    "#ones = torch.ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size\n",
    "xx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5000, 0.9000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using list in tensors\n",
    "x = torch.tensor([4.5, 0.9])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6584, 0.8033],\n",
      "        [0.7095, 0.0634]])\n",
      "tensor([[0.5352, 0.9000],\n",
      "        [0.4289, 0.0633]])\n",
      "tensor([[1.1936, 1.7033],\n",
      "        [1.1385, 0.1267]])\n",
      "tensor([[1.1936, 1.7033],\n",
      "        [1.1385, 0.1267]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "z = x + y\n",
    "c = torch.add(x,y)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2312e-01, -9.6701e-02],\n",
       "        [ 2.8060e-01,  1.1653e-04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub\n",
    "c = torch.sub(x,y)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3524, 0.7230],\n",
       "        [0.3043, 0.0040]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mul\n",
    "d = torch.mul(x,y)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3524, 0.7230],\n",
       "        [0.3043, 0.0040]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mul_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8683,  1.1111],\n",
       "        [ 2.3314, 15.7961]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# div\n",
    "f = torch.div(x,y)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6173, 0.6756, 0.2815],\n",
      "        [0.1124, 0.6523, 0.3367],\n",
      "        [0.2045, 0.8280, 0.7030],\n",
      "        [0.4342, 0.4095, 0.4542],\n",
      "        [0.0798, 0.1416, 0.1696]])\n",
      "tensor([0.6173, 0.1124, 0.2045, 0.4342, 0.0798])\n"
     ]
    }
   ],
   "source": [
    "# slicing\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8670, 0.0463, 0.7613, 0.3825],\n",
      "        [0.7688, 0.3683, 0.1480, 0.9635],\n",
      "        [0.2621, 0.9134, 0.4856, 0.1630],\n",
      "        [0.9640, 0.0265, 0.9761, 0.1613]])\n",
      "tensor([0.8670, 0.0463, 0.7613, 0.3825, 0.7688, 0.3683, 0.1480, 0.9635, 0.2621,\n",
      "        0.9134, 0.4856, 0.1630, 0.9640, 0.0265, 0.9761, 0.1613])\n"
     ]
    }
   ],
   "source": [
    "# reshaping\n",
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "y = x.view(16)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from numpy to tensor and vice versa\n",
    "import numpy as np\n",
    "\n",
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  np.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Calculation with Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1684,  0.8309, -0.3047], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True) #calculate the gradient with respect to X\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1684, 2.8309, 1.6953], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.4037, 16.0284,  5.7484], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y*y*2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.4037, 16.0284,  5.7484], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z = z.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) #dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7585, 15.0983,  2.2672])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preventing pytorch from tracking the gradient functions history\n",
    "#x.requires_grad_(False)\n",
    "#x.detach()\n",
    "#with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1696,  0.4677, -0.7878])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1696,  0.4677, -0.7878])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.detach()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1696, 2.4677, 1.2122])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1684, 2.8309, 1.6953], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model_output = (weights * 3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward pass: Compute loss\n",
    "- Compute local gradients\n",
    "- Backward pass: Compute dLoss / dWeights using the chain rule\n",
    "\n",
    "yhat = w * x\n",
    "\n",
    "Loss = (yhat - y)^2 = (wx -y)^2\n",
    "\n",
    "Forward pass: x = A, y=2, w=A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code implementation\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#forward pass and the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward pass\n",
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5)= 0.000\n",
      "epoch 1: w - 1.200, loss = 30.00000000\n",
      "epoch 2: w - 1.680, loss = 4.79999924\n",
      "epoch 3: w - 1.872, loss = 0.76800019\n",
      "epoch 4: w - 1.949, loss = 0.12288000\n",
      "epoch 5: w - 1.980, loss = 0.01966083\n",
      "epoch 6: w - 1.992, loss = 0.00314570\n",
      "epoch 7: w - 1.997, loss = 0.00050332\n",
      "epoch 8: w - 1.999, loss = 0.00008053\n",
      "epoch 9: w - 1.999, loss = 0.00001288\n",
      "epoch 10: w - 2.000, loss = 0.00000206\n",
      "Prediction after training: f(5)= 9.999\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward_pass(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE (Linear regression)\n",
    "def loss (y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "#gradient\n",
    "#MSE  = 1/N * (w*x - y)**2\n",
    "# dj/dw = 1/N 2x (w*x -y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5)= {forward_pass(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward_pass(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch+1}: w - {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5)= {forward_pass(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5)= 0.000\n",
      "epoch 1: w - 0.300, loss = 30.00000000\n",
      "epoch 11: w - 1.665, loss = 1.16278565\n",
      "epoch 21: w - 1.934, loss = 0.04506890\n",
      "epoch 31: w - 1.987, loss = 0.00174685\n",
      "epoch 41: w - 1.997, loss = 0.00006770\n",
      "epoch 51: w - 1.999, loss = 0.00000262\n",
      "epoch 61: w - 2.000, loss = 0.00000010\n",
      "epoch 71: w - 2.000, loss = 0.00000000\n",
      "epoch 81: w - 2.000, loss = 0.00000000\n",
      "epoch 91: w - 2.000, loss = 0.00000000\n",
      "epoch 101: w - 2.000, loss = 0.00000000\n",
      "epoch 111: w - 2.000, loss = 0.00000000\n",
      "epoch 121: w - 2.000, loss = 0.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 131: w - 2.000, loss = 0.00000000\n",
      "epoch 141: w - 2.000, loss = 0.00000000\n",
      "epoch 151: w - 2.000, loss = 0.00000000\n",
      "epoch 161: w - 2.000, loss = 0.00000000\n",
      "epoch 171: w - 2.000, loss = 0.00000000\n",
      "epoch 181: w - 2.000, loss = 0.00000000\n",
      "epoch 191: w - 2.000, loss = 0.00000000\n",
      "epoch 201: w - 2.000, loss = 0.00000000\n",
      "epoch 211: w - 2.000, loss = 0.00000000\n",
      "epoch 221: w - 2.000, loss = 0.00000000\n",
      "epoch 231: w - 2.000, loss = 0.00000000\n",
      "epoch 241: w - 2.000, loss = 0.00000000\n",
      "epoch 251: w - 2.000, loss = 0.00000000\n",
      "epoch 261: w - 2.000, loss = 0.00000000\n",
      "epoch 271: w - 2.000, loss = 0.00000000\n",
      "epoch 281: w - 2.000, loss = 0.00000000\n",
      "epoch 291: w - 2.000, loss = 0.00000000\n",
      "epoch 301: w - 2.000, loss = 0.00000000\n",
      "epoch 311: w - 2.000, loss = 0.00000000\n",
      "epoch 321: w - 2.000, loss = 0.00000000\n",
      "epoch 331: w - 2.000, loss = 0.00000000\n",
      "epoch 341: w - 2.000, loss = 0.00000000\n",
      "epoch 351: w - 2.000, loss = 0.00000000\n",
      "epoch 361: w - 2.000, loss = 0.00000000\n",
      "epoch 371: w - 2.000, loss = 0.00000000\n",
      "epoch 381: w - 2.000, loss = 0.00000000\n",
      "epoch 391: w - 2.000, loss = 0.00000000\n",
      "epoch 401: w - 2.000, loss = 0.00000000\n",
      "epoch 411: w - 2.000, loss = 0.00000000\n",
      "epoch 421: w - 2.000, loss = 0.00000000\n",
      "epoch 431: w - 2.000, loss = 0.00000000\n",
      "epoch 441: w - 2.000, loss = 0.00000000\n",
      "epoch 451: w - 2.000, loss = 0.00000000\n",
      "epoch 461: w - 2.000, loss = 0.00000000\n",
      "epoch 471: w - 2.000, loss = 0.00000000\n",
      "epoch 481: w - 2.000, loss = 0.00000000\n",
      "epoch 491: w - 2.000, loss = 0.00000000\n",
      "epoch 501: w - 2.000, loss = 0.00000000\n",
      "epoch 511: w - 2.000, loss = 0.00000000\n",
      "epoch 521: w - 2.000, loss = 0.00000000\n",
      "epoch 531: w - 2.000, loss = 0.00000000\n",
      "epoch 541: w - 2.000, loss = 0.00000000\n",
      "epoch 551: w - 2.000, loss = 0.00000000\n",
      "epoch 561: w - 2.000, loss = 0.00000000\n",
      "epoch 571: w - 2.000, loss = 0.00000000\n",
      "epoch 581: w - 2.000, loss = 0.00000000\n",
      "epoch 591: w - 2.000, loss = 0.00000000\n",
      "epoch 601: w - 2.000, loss = 0.00000000\n",
      "epoch 611: w - 2.000, loss = 0.00000000\n",
      "epoch 621: w - 2.000, loss = 0.00000000\n",
      "epoch 631: w - 2.000, loss = 0.00000000\n",
      "epoch 641: w - 2.000, loss = 0.00000000\n",
      "epoch 651: w - 2.000, loss = 0.00000000\n",
      "epoch 661: w - 2.000, loss = 0.00000000\n",
      "epoch 671: w - 2.000, loss = 0.00000000\n",
      "epoch 681: w - 2.000, loss = 0.00000000\n",
      "epoch 691: w - 2.000, loss = 0.00000000\n",
      "epoch 701: w - 2.000, loss = 0.00000000\n",
      "epoch 711: w - 2.000, loss = 0.00000000\n",
      "epoch 721: w - 2.000, loss = 0.00000000\n",
      "epoch 731: w - 2.000, loss = 0.00000000\n",
      "epoch 741: w - 2.000, loss = 0.00000000\n",
      "epoch 751: w - 2.000, loss = 0.00000000\n",
      "epoch 761: w - 2.000, loss = 0.00000000\n",
      "epoch 771: w - 2.000, loss = 0.00000000\n",
      "epoch 781: w - 2.000, loss = 0.00000000\n",
      "epoch 791: w - 2.000, loss = 0.00000000\n",
      "epoch 801: w - 2.000, loss = 0.00000000\n",
      "epoch 811: w - 2.000, loss = 0.00000000\n",
      "epoch 821: w - 2.000, loss = 0.00000000\n",
      "epoch 831: w - 2.000, loss = 0.00000000\n",
      "epoch 841: w - 2.000, loss = 0.00000000\n",
      "epoch 851: w - 2.000, loss = 0.00000000\n",
      "epoch 861: w - 2.000, loss = 0.00000000\n",
      "epoch 871: w - 2.000, loss = 0.00000000\n",
      "epoch 881: w - 2.000, loss = 0.00000000\n",
      "epoch 891: w - 2.000, loss = 0.00000000\n",
      "epoch 901: w - 2.000, loss = 0.00000000\n",
      "epoch 911: w - 2.000, loss = 0.00000000\n",
      "epoch 921: w - 2.000, loss = 0.00000000\n",
      "epoch 931: w - 2.000, loss = 0.00000000\n",
      "epoch 941: w - 2.000, loss = 0.00000000\n",
      "epoch 951: w - 2.000, loss = 0.00000000\n",
      "epoch 961: w - 2.000, loss = 0.00000000\n",
      "epoch 971: w - 2.000, loss = 0.00000000\n",
      "epoch 981: w - 2.000, loss = 0.00000000\n",
      "epoch 991: w - 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5)= 10.000\n"
     ]
    }
   ],
   "source": [
    "# using pytorch\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward_pass(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE (Linear regression)\n",
    "def loss (y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5)= {forward_pass(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward_pass(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients = backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w - {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5)= {forward_pass(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Design model (input, output size, forward pass)\n",
    "- Construct loss and optimizer\n",
    "- Training loop:\n",
    "-- forward pass: compute prediction\n",
    "-- backward pass: gradients\n",
    "-- update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5)= -0.416\n",
      "epoch 1: w = 0.186, loss = 31.72766304\n",
      "epoch 21: w = 1.697, loss = 0.12213358\n",
      "epoch 41: w = 1.751, loss = 0.08957390\n",
      "epoch 61: w = 1.766, loss = 0.07943748\n",
      "epoch 81: w = 1.780, loss = 0.07045934\n",
      "epoch 101: w = 1.793, loss = 0.06249586\n",
      "epoch 121: w = 1.805, loss = 0.05543232\n",
      "epoch 141: w = 1.816, loss = 0.04916724\n",
      "epoch 161: w = 1.827, loss = 0.04361024\n",
      "epoch 181: w = 1.837, loss = 0.03868131\n",
      "epoch 201: w = 1.846, loss = 0.03430945\n",
      "epoch 221: w = 1.855, loss = 0.03043175\n",
      "epoch 241: w = 1.864, loss = 0.02699225\n",
      "epoch 261: w = 1.872, loss = 0.02394151\n",
      "epoch 281: w = 1.879, loss = 0.02123556\n",
      "epoch 301: w = 1.886, loss = 0.01883546\n",
      "epoch 321: w = 1.893, loss = 0.01670661\n",
      "epoch 341: w = 1.899, loss = 0.01481842\n",
      "epoch 361: w = 1.905, loss = 0.01314358\n",
      "epoch 381: w = 1.910, loss = 0.01165805\n",
      "epoch 401: w = 1.916, loss = 0.01034042\n",
      "epoch 421: w = 1.921, loss = 0.00917174\n",
      "epoch 441: w = 1.925, loss = 0.00813513\n",
      "epoch 461: w = 1.930, loss = 0.00721566\n",
      "epoch 481: w = 1.934, loss = 0.00640015\n",
      "epoch 501: w = 1.937, loss = 0.00567677\n",
      "epoch 521: w = 1.941, loss = 0.00503518\n",
      "epoch 541: w = 1.945, loss = 0.00446609\n",
      "epoch 561: w = 1.948, loss = 0.00396132\n",
      "epoch 581: w = 1.951, loss = 0.00351360\n",
      "epoch 601: w = 1.954, loss = 0.00311647\n",
      "epoch 621: w = 1.956, loss = 0.00276425\n",
      "epoch 641: w = 1.959, loss = 0.00245183\n",
      "epoch 661: w = 1.961, loss = 0.00217473\n",
      "epoch 681: w = 1.964, loss = 0.00192892\n",
      "epoch 701: w = 1.966, loss = 0.00171091\n",
      "epoch 721: w = 1.968, loss = 0.00151754\n",
      "epoch 741: w = 1.970, loss = 0.00134602\n",
      "epoch 761: w = 1.971, loss = 0.00119389\n",
      "epoch 781: w = 1.973, loss = 0.00105896\n",
      "epoch 801: w = 1.975, loss = 0.00093928\n",
      "epoch 821: w = 1.976, loss = 0.00083311\n",
      "epoch 841: w = 1.977, loss = 0.00073895\n",
      "epoch 861: w = 1.979, loss = 0.00065544\n",
      "epoch 881: w = 1.980, loss = 0.00058136\n",
      "epoch 901: w = 1.981, loss = 0.00051565\n",
      "epoch 921: w = 1.982, loss = 0.00045737\n",
      "epoch 941: w = 1.983, loss = 0.00040568\n",
      "epoch 961: w = 1.984, loss = 0.00035983\n",
      "epoch 981: w = 1.985, loss = 0.00031915\n",
      "Prediction after training: f(5)= 9.971\n"
     ]
    }
   ],
   "source": [
    "# using pytorch\n",
    "\n",
    "# f = w * x\n",
    "# f = 2 * x\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5)= {model(X_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients = backward pass\n",
    "    l.backward() #dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5)= {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "- prepare data\n",
    "- model\n",
    "- loss and optimizer\n",
    "- training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 1628.7839\n",
      "epoch: 20, loss = 1240.9712\n",
      "epoch: 30, loss = 971.7648\n",
      "epoch: 40, loss = 784.6990\n",
      "epoch: 50, loss = 654.5821\n",
      "epoch: 60, loss = 563.9911\n",
      "epoch: 70, loss = 500.8616\n",
      "epoch: 80, loss = 456.8307\n",
      "epoch: 90, loss = 426.0947\n",
      "epoch: 100, loss = 404.6224\n",
      "epoch: 110, loss = 389.6102\n",
      "epoch: 120, loss = 379.1071\n",
      "epoch: 130, loss = 371.7537\n",
      "epoch: 140, loss = 366.6020\n",
      "epoch: 150, loss = 362.9906\n",
      "epoch: 160, loss = 360.4575\n",
      "epoch: 170, loss = 358.6799\n",
      "epoch: 180, loss = 357.4316\n",
      "epoch: 190, loss = 356.5548\n",
      "epoch: 200, loss = 355.9384\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_numpy, y_numpy = datasets.make_regression(n_samples=300, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "\n",
    "y = y.view(y.shape[0], 1) #view is a pytorch method that does resampling\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # forward pass\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if  (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# plotting it\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss= 0.4578\n",
      "epoch: 20, loss= 0.4035\n",
      "epoch: 30, loss= 0.3658\n",
      "epoch: 40, loss= 0.3378\n",
      "epoch: 50, loss= 0.3160\n",
      "epoch: 60, loss= 0.2984\n",
      "epoch: 70, loss= 0.2836\n",
      "epoch: 80, loss= 0.2711\n",
      "epoch: 90, loss= 0.2603\n",
      "epoch: 100, loss= 0.2507\n",
      "epoch: 110, loss= 0.2423\n",
      "epoch: 120, loss= 0.2347\n",
      "epoch: 130, loss= 0.2279\n",
      "epoch: 140, loss= 0.2216\n",
      "epoch: 150, loss= 0.2159\n",
      "epoch: 160, loss= 0.2107\n",
      "epoch: 170, loss= 0.2058\n",
      "epoch: 180, loss= 0.2013\n",
      "epoch: 190, loss= 0.1972\n",
      "epoch: 200, loss= 0.1933\n",
      "accuracy = 0.9825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# data\n",
    "breast = datasets.load_breast_cancer()\n",
    "X, y = breast.data, breast.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale\n",
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # empty gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss= {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_class = y_predicted.round()\n",
    "    accuracy = y_predicted_class.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
